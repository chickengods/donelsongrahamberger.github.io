---
layout: post
title: Luna Lander RL
summary: I used Actor-Critic reinforcement learning to solve the Luna Lander problem.
card_image: /assets/images/lunar_lander.gif
t_use: true
---
The best resource to get started with reinforcement learning is [OpenAI's gym](https://gym.openai.com/). Take it straight from their website: _Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball._ They abstracted away all the work of having to code environments to test reinforcement learning algorithms and created a framework for third parties to create their own environments. All you need to worry about it handling the agent you want to teach. I highly recommend anyone that is interested in reinforcement learning to check it out and play around with it. I would start with classic control problems, like CartPole-v1. My only complaint is that there isn't any environments for multi-agent reinforcement learning.

After taking cs639, Sequential Decision Making and Learning, I wanted to test what I learned by tackling the [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) environment. In this environment, we are tasked to teach a lander to descend and land safely. We are able to control its 3 thrusters left, right and bottom, in a binary fashion. My approach to this problem was a simple implantation of [Actor-Critic](https://arxiv.org/pdf/1611.01224.pdf) with batch learning. Check out my averaged score over the course of my training.

![Averaged Score Over Training](/assets/images/iinasd2.png)

It took a long time but eventually it was able to get a somewhat good policy. It was able to land correctly about 30% of the time. I think if I went back and tuned my hyperparameters I could get more consistent results. One thing that you will notice in this graphs, periodically it does worse all of a sudden. This is cause by me, trying to encourage more exploration by increasing the chance of choosing a random action. At this point I have to come clean...the gif at the top isn't of my agent. I forgot to record it when I did this so I took one off the internet. My agent learned a unique approach to this problem. It would swing side to side because it learned that by coming at the right angle it could kill its momentum by hitting hits leg against the ground, allowing it to descend at a faster rate. It was a high risk high reward policy. When it worked it could maximize its reward, however when it failed it would tip over crashing on its head, causing some of the lowest scores possible. This explains why it only barely averaged above 0 at the end. This is a great example of why I love reinforcement learning, it is surprising when it learns to cut corners by exploiting the game engine.
