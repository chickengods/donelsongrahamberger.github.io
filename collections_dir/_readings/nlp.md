---
layout: reading 
title: Natural Language Processing
t_use: true
order: 4
---

- [Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory](https://arxiv.org/abs/2305.17144), May 30 2023
- [First-principles on AI scaling](https://dynomight.net/scaling/), May 14 2023
- [Why didn't we get GPT-2 in 2005?](https://dynomight.net/gpt-2/), May 14 2023
- [The GTP-3 Architectrure, on a Napkin](https://dugas.ch/artificial_curiosity/GPT_architecture.html)
- [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)
- [Scaling Transformer to 1M tokens and beyong with RMT](https://arxiv.org/abs/2304.11062)
- [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)
- [How to run your own LLM GPT](https://blog.rfox.eu/en/Programming/How_to_run_your_own_LLM_GPT.html)
- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/?ref=txt.cohere.com)
- [Google "We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
